{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iam0g6cjT2c",
        "colab_type": "text"
      },
      "source": [
        "#Data Generation - News Articles Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x24LcS4csHKm",
        "colab_type": "text"
      },
      "source": [
        "##Scroll and History News Network articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC8amFxpmQ7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from multiprocessing import Pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hwpbvCdjac3",
        "colab_type": "code",
        "outputId": "f2bb5979-6653-4d2d-ca2f-f07b3bc057b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re,datetime\n",
        "start_time = time.time()\n",
        "\n",
        "class Corpus:\n",
        "    def __init__(self,web_url,start,end,filename,fieldnames):\n",
        "      self.web_url=web_url\n",
        "      self.start=start\n",
        "      self.end=end\n",
        "      self.rawdata=[]\n",
        "      self.article_id=[]\n",
        "      self.headline = []\n",
        "      self.dates = []\n",
        "      self.category = []\n",
        "      self.author = []\n",
        "      self.articles = []\n",
        "      self.filename=filename\n",
        "      self.fieldnames=fieldnames\n",
        "\n",
        "    def gen_corpus(self):\n",
        "      for i in range(self.start,self.end): #article numbers\n",
        "          self.article_id.append(str(int(i)))\n",
        "          rd=requests.get(self.web_url+'article/'+str(int(i))).text\n",
        "          self.rawdata.append(rd)\n",
        "      print(len(self.rawdata))\n",
        "      print(\"%s seconds\" % (time.time() - start_time))\n",
        "\n",
        "    def gen_corpus_attr(self):\n",
        "\n",
        "      for i in range (len(self.rawdata)):\n",
        "          # Turn page into BeautifulSoup object to access HTML tags\n",
        "          soup = bs(self.rawdata[i])\n",
        "\n",
        "          #to get headline\n",
        "          try:\n",
        "              head = soup.find('h1').get_text()\n",
        "          except:\n",
        "              head = None\n",
        "          #to get date  (the date in the webpeg was in the info class with some other info, so it needed to be extracted)\n",
        "          try:\n",
        "              date = soup.find_all(class_=\"article-info\")\n",
        "              datetext = [tag.get_text().strip() for tag in date]\n",
        "              date = datetext[-1]\n",
        "          except:\n",
        "              date = None\n",
        "          #to get category\n",
        "          try:\n",
        "              cat = soup.find(style=\"color:#777;\").get_text().strip()\n",
        "          except:\n",
        "              cat = None\n",
        "          #to get other\n",
        "          try:\n",
        "              auth = soup.find(class_=\"byline\").get_text()\n",
        "          except:\n",
        "              auth = None\n",
        "\n",
        "          #to extract text\n",
        "          p_tags = soup.find_all('p')\n",
        "          # Get the text from each of the \"p\" tags and strip surrounding whitespace.\n",
        "          p_tags_text = [tag.get_text().replace('\\xa0', ' ').strip('\\t') for tag in p_tags] #to remove non-breaking spaces from the document\n",
        "          article = ' '.join(p_tags_text)\n",
        "          self.headline.append(str(head))\n",
        "          self.dates.append(str(date))\n",
        "          self.category.append(str(cat))\n",
        "          self.author.append(str(auth))\n",
        "          self.articles.append(str(article))\n",
        "\n",
        "    def gen_corpusfile(self):\n",
        "      f = open(self.filename, \"w\")\n",
        "      writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
        "      writer.writeheader()\n",
        "      f.close()\n",
        "      with open(self.filename, 'a') as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(zip(self.article_id, self.dates, self.author, self.category, self.headline, self.articles))\n",
        "\n",
        "c1_scroll=Corpus('https://scroll.in/',896000,904500,'scrollnews.csv',['Article_id', 'Date', 'Author', 'Topic', 'Headline', 'Article content'])\n",
        "c2_hnn=Corpus('https://historynewsnetwork.org/',170100,172101,'HNN.csv',['Article_id', 'Date', 'Author', 'Topic', 'Headline','Article content'])\n",
        "\n",
        "c2_hnn.gen_corpus()\n",
        "c2_hnn.gen_corpus_attr()\n",
        "c2_hnn.gen_corpusfile()\n",
        "#896000 to 904500\n",
        "#170100 to 172101"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2001\n",
            "244.63873147964478 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twG3OecwmKhe",
        "colab_type": "code",
        "outputId": "754802d2-d901-4be9-a0b7-0ef6e86da5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "c1_scroll.gen_corpus()\n",
        "c1_scroll.gen_corpus_attr()\n",
        "c1_scroll.gen_corpusfile()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8500\n",
            "18478.092730998993 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXQr76-WS8dX",
        "colab_type": "code",
        "outputId": "8bb5f033-2e14-4b5e-eda5-373a3b795754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "dfhnn=pd.read_csv(c2_hnn.filename)\n",
        "dfhnn.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article_id</th>\n",
              "      <th>Date</th>\n",
              "      <th>Author</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Article content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>172096</td>\n",
              "      <td>5/27/2019</td>\n",
              "      <td>by Jim Sleeper</td>\n",
              "      <td>Roundup</td>\n",
              "      <td>Billionaires can't fix college: Jim Sleeper on...</td>\n",
              "      <td>Jim Sleeper is a lecturer in political scienc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>172097</td>\n",
              "      <td>5/8/2019</td>\n",
              "      <td>by Ralph Seliger</td>\n",
              "      <td>Roundup</td>\n",
              "      <td>My Zionism is Personal and Complicated</td>\n",
              "      <td>Ralph Seliger specializes in writing about Is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>172098</td>\n",
              "      <td>6/2/19</td>\n",
              "      <td>by Donald J. Fraser</td>\n",
              "      <td>News at Home</td>\n",
              "      <td>A Brief History of the Theory Trump and Barr U...</td>\n",
              "      <td>Donald J. Fraser has spent a lifetime working...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>172099</td>\n",
              "      <td>6/2/19</td>\n",
              "      <td>by Gregory Sumner</td>\n",
              "      <td>News Abroad</td>\n",
              "      <td>Remembering Rome's Liberation</td>\n",
              "      <td>Gregory Sumner is author of Detroit in WWII a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>172100</td>\n",
              "      <td>6/2/19</td>\n",
              "      <td>by William Lambers</td>\n",
              "      <td>News at Home</td>\n",
              "      <td>D-Day 75 Years Later and the Quest for Peace</td>\n",
              "      <td>William Lambers is the author of the Road to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Article_id  ...                                    Article content\n",
              "1996      172096  ...   Jim Sleeper is a lecturer in political scienc...\n",
              "1997      172097  ...   Ralph Seliger specializes in writing about Is...\n",
              "1998      172098  ...   Donald J. Fraser has spent a lifetime working...\n",
              "1999      172099  ...   Gregory Sumner is author of Detroit in WWII a...\n",
              "2000      172100  ...   William Lambers is the author of the Road to ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foXJDcBU4f1J",
        "colab_type": "code",
        "outputId": "81b364a9-9a35-47ae-af46-5751a4e43793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "dfscroll=pd.read_csv(c1_scroll.filename)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article_id</th>\n",
              "      <th>Date</th>\n",
              "      <th>Author</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Article content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8495</th>\n",
              "      <td>904495</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Government is using litigation to intimidate C...</td>\n",
              "      <td>Former Information Commissioner Sridhar Achar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8496</th>\n",
              "      <td>904496</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Telangana polls: Congress urges EC to act agai...</td>\n",
              "      <td>The Congress on Tuesday urged the Election Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8497</th>\n",
              "      <td>904497</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Jose Mourinho denies stating it would be a mir...</td>\n",
              "      <td>Jose Mourinho has acknowledged Manchester Uni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8498</th>\n",
              "      <td>904498</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Tottenham fan escapes racism charge after bana...</td>\n",
              "      <td>The Tottenham Hotspur fan who threw a banana ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8499</th>\n",
              "      <td>904499</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Sri Lanka: Rajapaksa moves SC against order re...</td>\n",
              "      <td>Sri Lankan Prime Minister Mahinda Rajapaksa o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Article_id  ...                                    Article content\n",
              "8495      904495  ...   Former Information Commissioner Sridhar Achar...\n",
              "8496      904496  ...   The Congress on Tuesday urged the Election Co...\n",
              "8497      904497  ...   Jose Mourinho has acknowledged Manchester Uni...\n",
              "8498      904498  ...   The Tottenham Hotspur fan who threw a banana ...\n",
              "8499      904499  ...   Sri Lankan Prime Minister Mahinda Rajapaksa o...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfa6Oeq6sAQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfscroll.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQjDkftWwDCz",
        "colab_type": "text"
      },
      "source": [
        "##News Articles from The Verge, ESPN and Sports Bible\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdRKy19tq5nY",
        "colab_type": "code",
        "outputId": "bbcc5117-5e26-4ed3-f571-f7c35a9ad859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "!pip install newspaper3k\n",
        "import newspaper\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_newspaper(url):\n",
        "    source = newspaper.build(url, memoize_articles=False)\n",
        "    return source\n",
        "\n",
        "def see_urls(source):\n",
        "    print('{} articles fetched'.format(source.size()))\n",
        "    for article in source.articles:\n",
        "        print(article.url)\n",
        "\n",
        "def batch_download_articles(source):\n",
        "    start_time = time.time()\n",
        "    newspaper.news_pool.set([source])\n",
        "    newspaper.news_pool.join()\n",
        "    print('Downloaded {} articles in {} seconds'.format(source.size(),time.time() - start_time))\n",
        "\n",
        "def parse_articles(source):\n",
        "    start_time = time.time()\n",
        "    for article in source.articles:\n",
        "          article.parse()\n",
        "    print('Parsed {} articles in {} seconds'.format(source.size(),time.time() - start_time))\n",
        "          \n",
        "def clean(article):\n",
        "    article = re.sub('\\s+',' ',article)\n",
        "    return article\n",
        "          \n",
        "def create_DataFrame(source, category):\n",
        "    df = pd.DataFrame(data={\"Article_id\":[None for i in range(source.size())]\n",
        "                                 ,\"Date\": [str(article.publish_date) for article in source.articles]\n",
        "                                 ,\"Author\": [article.authors for article in source.articles]\n",
        "                                 ,\"Topic\": [category for i in range(source.size())]\n",
        "                                 ,\"Headline\": [article.title for article in source.articles]\n",
        "                                 ,\"Article content\": [clean(article.text) for article in source.articles]\n",
        "                                 })\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.9)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inv5LOsBq5kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tech articles from The Verge\n",
        "verge = fetch_newspaper('https://www.theverge.com/')\n",
        "#look for any unnecessary articles\n",
        "see_urls(verge)\n",
        "\n",
        "#remove unnecessary articles\n",
        "comment_urls = [article.url for article in verge.articles if 'comments' in article.url.split(\"#\")]\n",
        "for url in comment_urls:\n",
        "    for article in verge.articles:\n",
        "        if url == article.url:\n",
        "            verge.articles.remove(article)\n",
        "\n",
        "#see the changes\n",
        "see_urls(verge)\n",
        "\n",
        "#download and parse all the articles to extract all the relevant info\n",
        "batch_download_articles(verge)\n",
        "parse_articles(verge)\n",
        "\n",
        "#create a pandas dataframe and save as .csv file\n",
        "verge_df = create_DataFrame(verge, 'technology')\n",
        "verge_df.to_csv('verge_articles.csv')\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "#sports articles from ESPN\n",
        "espn = fetch_newspaper('https://www.espn.in/')\n",
        "#look for any unnecessary articles\n",
        "see_urls(espn)\n",
        "\n",
        "#remove unnecessary articles\n",
        "videos = [article.url for article in espn.articles if 'video' in article.url.split(\"/\")]\n",
        "for url in videos:\n",
        "    for article in espn.articles:\n",
        "        if url == article.url:\n",
        "            espn.articles.remove(article)\n",
        "\n",
        "#see the changes\n",
        "see_urls(espn)\n",
        "\n",
        "#download and parse all the articles to extract all the relevant info\n",
        "batch_download_articles(espn)\n",
        "parse_articles(espn)\n",
        "\n",
        "#create a pandas dataframe and save as .csv file\n",
        "espn_df = create_DataFrame(espn, 'sports')\n",
        "espn_df.to_csv('espn_articles.csv')\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "#more sports articles from Sport Bible\n",
        "sport_bible = fetch_newspaper('https://www.sportbible.com/')\n",
        "#look for any unnecessary articles\n",
        "# see_urls(sport_bible)\n",
        "print('{} Articles available'.format(sport_bible.size()))\n",
        "\n",
        "#remove unnecessary articles\n",
        "unnecessary = [article.url for article in sport_bible.articles if 'pages' in article.url.split(\"/\")]\n",
        "for url in unnecessary:\n",
        "    for article in sport_bible.articles:\n",
        "        if url == article.url:\n",
        "            sport_bible.articles.remove(article)\n",
        "            \n",
        "#see the changes\n",
        "# see_urls(sport_bible)\n",
        "print('{} Articles after deletion'.format(sport_bible.size()))\n",
        "\n",
        "#download and parse all the articles to extract all the relevant info\n",
        "print('Downloading articles...')\n",
        "batch_download_articles(sport_bible)\n",
        "print('Parsing Articles')\n",
        "parse_articles(sport_bible)\n",
        "\n",
        "#create a pandas dataframe and save as .csv file\n",
        "sport_bible_df = create_DataFrame(sport_bible, 'sports')\n",
        "print('.csv file created')\n",
        "sport_bible_df.to_csv('sport_bible_articles.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atei7b7Xq5h9",
        "colab_type": "code",
        "outputId": "e59b136c-ca8e-48de-ee37-a75914b957d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "verge = pd.read_csv('verge_articles.csv')\n",
        "espn = pd.read_csv('espn_articles.csv')\n",
        "sport_bible = pd.read_csv('sport_bible_articles.csv')\n",
        "\n",
        "df = verge.copy()\n",
        "df = df.append([espn,sport_bible])\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "df = df.reset_index()\n",
        "df = df.drop(columns=['index'])\n",
        "\n",
        "print(df)\n",
        "df.to_csv('compiled_news.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Article_id  ...                                    Article content\n",
            "0            NaN  ...  Twitter is testing a new feature that lets use...\n",
            "1            NaN  ...  Only the best deals on Verge-approved gadgets ...\n",
            "2            NaN  ...  President Trump wants to create a panel that w...\n",
            "3            NaN  ...  I forgot to write about Westworld after its th...\n",
            "4            NaN  ...  They’re pulling the plug on the New York Auto ...\n",
            "...          ...  ...                                                ...\n",
            "2144         NaN  ...  Even though the World Cup is over and we're le...\n",
            "2145         NaN  ...  Social media was ablaze with comments about Ki...\n",
            "2146         NaN  ...  Ferrari driver Kimi Raikkonen is getting a lot...\n",
            "2147         NaN  ...  Jack Kenmare Jack Kenmare is a writer at SPORT...\n",
            "2148         NaN  ...  The manager of former F1 champion Michael Schu...\n",
            "\n",
            "[2149 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWydNFZUcYpo",
        "colab_type": "code",
        "outputId": "263b672d-aa3d-4e38-a949-a969e6fee9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tGCEfLiUXBJ",
        "colab_type": "text"
      },
      "source": [
        "##Final corpus - combine news from all sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3ocjQK57Swn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to remove missing values from the final news corpus\n",
        "\n",
        "import pandas as pd\n",
        "missing_values = [\"n/a\", \"na\", \"--\",\"None\",\"nan\",\"nil\",\"NaN\",\"Null\",\"none\"]\n",
        "scrollarticles1 = pd.read_csv(c1_scroll.filename, na_values = missing_values)\n",
        "del scrollarticles1['Unnamed: 0']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwWwqcgj7S2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HNNarticles = pd.read_csv(c2_hnn.filename, na_values = missing_values)\n",
        "del HNNarticles1['Unnamed: 0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYjJIuF27ZL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compiledarticles = pd.read_csv('compiled_news.csv', na_values = missing_values)\n",
        "del compiledarticles['Unnamed: 0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XqbRDxtUWYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "frames = [scrollarticles, scrollarticles1, HNNarticles1, compiledarticles1]\n",
        "corpus = pd.concat(frames)\n",
        "corpus = corpus[corpus['Article content'].notna()]\n",
        "print(len(corpus))\n",
        "corpus.columns.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqyVAwhD_ecm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus.to_csv(\"corpus_final.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2eb-WO__elg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('corpus_final.csv', index_col=[0])\n",
        "print(len(data))\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}